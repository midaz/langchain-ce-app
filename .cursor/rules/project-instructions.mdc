---
description: 
globs: 
alwaysApply: false
---
# LangChain Support Query Router & Responder – Build Guide (with Severity & Escalation)

## 1. What you are building

A compact AI co‑pilot that:

* **Triages** incoming GitHub issues (bug, feature, support)
* **Assigns severity** (1–4) for internal workflows
* **Routes or escalates** high‑urgency tickets
* **Answers** support questions using docs

**Input**: Issue title + body (from GitHub)
**Output**: JSON containing either an **escalation response** or a **self‑help answer** plus metadata (type, severity, trace URL)

Each pipeline run logs to **LangSmith** for metrics on routing accuracy, correctness, latency, and hallucination.

---

## 2. High-level flow

```text
User question
   ├─► IssueTypeChain → issue_type ∈ {bug, feature request, support question}
   │      ├ if ≠ support  ──► SeverityChain → severity → return “file as bug/feature” template + trace
   │      └ if = support
   │
   ├─► SeverityChain → severity ∈ {1,2,3,4}
   │      ├ if severity ≤ 2 ──► return internal escalation template + trace
   │      └ else (3 or 4)
   │
   ├─► CategoryChain → category ∈ {installation, chains, agents, memory, retrieval}
   │
   ├─► Retriever (docs) → context (top‑k snippets)
   │
   └─► AnswerChain (category + context + query) → final self‑help JSON + trace
```

---

## 3. Prompts & Chains

### 3.1 IssueTypeChain

```text
System: You are a triage assistant.
Task: Classify the issue as exactly one of:
 - bug report
 - feature request
 - support question
Input: {query}
Return a single label.
```

### 3.2 SeverityChain

```text
System: You are a severity classifier. Definitions:
 1) Security or production outage
 2) Major bug preventing usage
 3) Minor bug or investigate
 4) Docs, enhancement, or info request
Input: {query}
Return JUST 1, 2, 3, or 4.
```

**Severity Rubric (for reference):**

* **Severity 1**: Critical production outage or security issue (e.g. end-user inaccessible service)
* **Severity 2**: Major bug blocking regular usage (e.g. enterprise feature broken)
* **Severity 3**: Minor bug or investigation needed (e.g. analytics wrong, non-blocking)
* **Severity 4**: Documentation, enhancement, or informational request (no negative impact)

### 3.3 CategoryChain CategoryChain

```text
System: You are a support router.
Task: Given a support question, pick one category:
 installation, chains, agents, memory, retrieval.
Input: {query}
Return JUST the category.
```

### 3.4 Retriever

* Use your vectorstore of selected docs (e.g. Quickstart, Academy Module 2, examples folder).
* Return top‑`k` chunks for grounding (start with `k=3`).

### 3.5 AnswerChain

```text
System: You are LangChain support.
Input variables: category, context, query
Prompt:
"""
Category: {category}
Context snippets:
{context}
Answer the user in 4–6 sentences. Include ONE markdown link to docs. """
```

---

## 4. Conditional Responses

* **Bugs/Feature Requests**: return

  ```json
  {
    "issue_type": "bug report",
    "severity": 3,
    "message": "Please file this under Bug or Enhancement in our repository.",
    "trace_url": "..."
  }
  ```

* **High-severity support (1–2)**: return

  ```json
  {
    "issue_type": "support question",
    "severity": 1,
    "escalation": "This looks urgent. I’m escalating to our on‑call engineering team now and will update you within 1 hour.",
    "trace_url": "..."
  }
  ```

* **Normal support (3–4)**: run CategoryChain → Retriever → AnswerChain and return

  ```json
  {
    "issue_type": "support question",
    "severity": 4,
    "category": "installation",
    "answer": "Here’s how to install... [link]",  
    "trace_url": "..."
  }
  ```

---

## 5. Step-by-step build

1. **Dataset (1h)**

   * Scrape and normalize GitHub labels → JSONL with fields: `query`, `ground_truth`, `labels`, `severity` (1–4).
2. **Select docs (1h)**

   * Choose Quickstart, Academy Module 2, or `/examples` (\~20 pages).
   * Build a vectorstore with `RecursiveCharacterTextSplitter(chunk_size=1000, overlap=150)` and small batches.
3. **Implement Chains (3h)**

   * Define LLMChains for IssueType, Severity, Category, Answer.
   * Use `RetrievalQA` or manual retriever.invoke() for context.
   * Wrap calls in `LangChainTracer`.
4. **Conditional logic (30m)**

   * If issue\_type ≠ support, stub bug/feature flow.
   * If severity ≤ 2, return escalation template.
   * Else, proceed to support flow.
5. **Optional Streamlit UI (1h)**

   * Simple form: input query → show JSON output + `[Trace]` link.
6. **LangSmith experiment (1h)**

   * Upload dataset; select `triage_and_answer` runnable.
   * Add evaluators: routing, severity match, correctness (LLM judge).
   * Run and export metrics.
7. **Friction Log (30m)**

   * Capture rate-limit, prompt tweaks, trace setup.

---

## 6. Deliverables

* `scripts/build_dataset.py`
* `router.py` with full pipeline + conditions
* `support_router_demo.py` (UI)
* `data/issues.jsonl`
* `friction_log.md`
* `requirements.txt`, `README.md`
* Public LangSmith project link, experiment screenshots
* Optional Loom/GIF demo

---

## 7. Cursor guidance

Paste this file into Cursor. When prompting Cursor:

* **“Help implement the SeverityChain”** → Cursor opens `router.py` and scaffolds that chain.
* **“Add escalation branch”** → Cursor inserts the `if severity ≤ 2` block.
* **“Optimize vectorstore build”** → Cursor refactors the split/embed code.
* **“Write evaluator for severity”** → Cursor adds a string-match evaluator in experiment YAML.

Follow these steps to finish in \~8 hours and showcase end‑to‑end customer engineering skills.

Good luck!
